{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4655f512-a374-464f-8d31-65a22c5273da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97555d9c-1499-43b6-b0f7-d07e2d6f88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bert4torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea27b0-33ae-4595-802e-8ef302a88988",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bdd08e-b199-42e7-947e-ff769294e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b762729-473e-4221-ac29-7bcc148179be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c9f7f-c40c-4915-a60f-445e25640dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7cef8e-d787-44e6-800e-c9740743cb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install safetensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e236b91-d1e1-4988-8e57-ce71cb339d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 导入pandas库，用于数据处理和分析\n",
    "import pandas as pd\n",
    "# 从sklearn.model_selection导入train_test_split函数，用于划分训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 从collections导入Counter类，用于统计对象出现的次数\n",
    "from collections import Counter\n",
    "# 从matplotlib.font_manager导入FontProperties类，用于字体管理\n",
    "from matplotlib.font_manager import FontProperties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f3d16de-dfad-4c6d-8b6f-cd7961ffdf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从'/root/data/train_set.csv'路径读取训练集数据，使用制表符('\\t')作为字段分隔符\n",
    "train_df = pd.read_csv('/root/data/train_set.csv', sep='\\t')\n",
    "# 从'/root/data/test_a.csv'路径读取测试集数据，使用制表符('\\t')作为字段分隔符\n",
    "test_df = pd.read_csv('/root/data/test_a.csv', sep='\\t')\n",
    "# 给测试集添加一个名为'label'的列，其所有值都设为0\n",
    "test_df['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfd4fe64-faff-4f0f-9ac8-35161962f4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2967 6758 339 2021 1854 3731 4109 3792 4149 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>4464 486 6352 5619 2465 4802 1452 3137 5778 54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7346 4068 5074 3747 5681 6093 1777 2226 7354 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>7159 948 4866 2109 5520 2490 211 3956 5520 549...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3646 3055 3055 2490 4659 6065 3370 5814 2465 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      2  2967 6758 339 2021 1854 3731 4109 3792 4149 15...\n",
       "1     11  4464 486 6352 5619 2465 4802 1452 3137 5778 54...\n",
       "2      3  7346 4068 5074 3747 5681 6093 1777 2226 7354 6...\n",
       "3      2  7159 948 4866 2109 5520 2490 211 3956 5520 549...\n",
       "4      3  3646 3055 3055 2490 4659 6065 3370 5814 2465 5..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 显示训练集数据的前5行\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aca3e78c-86c1-4da4-a72f-a9ea304f6bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取训练集数据的行数\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcb41112-b548-4266-b475-1f6c5bf2623c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5120, str)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取训练集中'text'列的第一个元素的长度和类型\n",
    "len(train_df['text'][0]), type(train_df['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b3a5c8-e9a3-4b52-ae04-8f21597326e0",
   "metadata": {},
   "source": [
    "## 学术加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d89ab23-7b28-4e5b-8dd9-25c0cf64e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入subprocess模块，用于执行外部命令\n",
    "import subprocess\n",
    "# 导入os模块，用于操作环境变量\n",
    "import os\n",
    "\n",
    "# 执行外部命令，source一个脚本文件并筛选包含'proxy'的环境变量，捕获命令的输出\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "# 获取命令的标准输出\n",
    "output = result.stdout\n",
    "# 遍历输出的每一行\n",
    "for line in output.splitlines():\n",
    "    # 如果行中包含'='符号\n",
    "    if '=' in line:\n",
    "        # 分割变量名和值\n",
    "        var, value = line.split('=', 1)\n",
    "        # 将变量名和值设置到当前环境变量中\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58a4866-5562-4756-a85b-82cba3cbda3f",
   "metadata": {},
   "source": [
    "## 重新训练分词器 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c2482-f2da-43cf-bb07-840dd107ad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入AutoTokenizer类\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 加载预训练的分词器\n",
    "old_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# 定义获取训练语料库的函数，使用生成器分批次处理数据\n",
    "def get_training_corpus():\n",
    "    # 以1000为步长遍历训练数据\n",
    "    for i in range(0, len(train_df), 1000):\n",
    "        # 生成当前批次的数据\n",
    "        yield train_df[\"text\"][i : i + 1000]\n",
    "\n",
    "# 使用旧的分词器的词表作为基础，在新的语料库上训练新的分词器\n",
    "new_tokenizer = old_tokenizer.train_new_from_iterator(get_training_corpus(), 10000)\n",
    "\n",
    "# 保存新的分词器到指定目录\n",
    "new_tokenizer.save_pretrained('./new_tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e22668-e839-4bc9-8313-a29994007dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取训练集的第一条数据，并截取前十个单词\n",
    "first_text = ' '.join(train_df.iloc[0]['text'].split()[:10])\n",
    "\n",
    "# 打印原始文本\n",
    "print(\"Original text:\", first_text)\n",
    "# 使用旧分词器进行分词，并打印结果\n",
    "print(\"Old tokenizer:\")\n",
    "print(old_tokenizer.tokenize(first_text))\n",
    "# 使用新分词器进行分词，并打印结果\n",
    "print(\"New tokenizer:\")\n",
    "print(new_tokenizer.tokenize(first_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fffce91-b43b-4c38-8817-fcc84c713294",
   "metadata": {},
   "source": [
    "## 预处理数据集，方便MLM训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d5830db-73fb-4778-97bf-2f83a1d6f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入AutoTokenizer类\n",
    "from transformers import AutoTokenizer\n",
    "# 导入Dataset类\n",
    "from datasets import Dataset\n",
    "\n",
    "# 加载重新训练好的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained('./new_tokenizer/')\n",
    "\n",
    "# 定义分词函数，直接截断超过最大长度的文本\n",
    "def tokenize_function(examples):\n",
    "    # 对输入的文本进行分词，设置填充到最大长度，超过最大长度则截断，最大长度设为512\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e5663b-86c3-4e5c-94a1-1afd42f0e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从pandas DataFrame创建数据集\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "\n",
    "# 使用定义的分词函数对数据集进行分词处理，并移除原始文本和标签列\n",
    "tokenized_datasets = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "\n",
    "# 现在tokenized_datasets已经准备好用于MLM预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdde482-eba1-494d-b7e7-ba4ad36d7586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入Dataset类和load_from_disk函数\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "# 将处理后的数据集保存到磁盘\n",
    "tokenized_datasets.save_to_disk(\"tokenized_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6ce95e6-c6ed-4a67-a080-3339e6b1b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入load_from_disk函数\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# 从磁盘加载处理后的数据集\n",
    "tokenized_datasets = load_from_disk(\"tokenized_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f0128d4-ec02-4eb0-90ac-eb98e54a5fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc3e833-fce9-490c-aea4-58f080bb5473",
   "metadata": {},
   "source": [
    "## MLM训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9dffda2-bc02-42a6-9a59-ae87dac97eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入os模块，用于操作系统功能，如环境变量、文件路径等\n",
    "import os\n",
    "# 导入torch模块，PyTorch框架的主模块，用于深度学习和张量计算\n",
    "import torch\n",
    "# 从transformers库导入AutoTokenizer和AutoModelForMaskedLM类，用于加载预训练的分词器和遮蔽语言模型\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "# 从transformers库导入DataCollatorForLanguageModeling类，用于数据整理，特别是为语言模型准备数据\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "# 从transformers库导入Trainer和TrainingArguments类，用于模型训练和设置训练参数\n",
    "from transformers import Trainer, TrainingArguments\n",
    "# 从datasets库导入Dataset类，用于处理和准备数据集\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "457a7397-4acf-4283-a5c3-f156a9c4487f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current device: 0\n",
      "Device count: 1\n",
      "Device name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# 检查CUDA可用性\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"Device count:\", torch.cuda.device_count())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# 清理CUDA缓存\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23f56d97-e3d2-4544-a462-0727b7ac863c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLM training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28120' max='28120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28120/28120 5:08:56, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.411500</td>\n",
       "      <td>3.005223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.417700</td>\n",
       "      <td>2.271334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.172700</td>\n",
       "      <td>2.068398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.063200</td>\n",
       "      <td>1.965433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.009800</td>\n",
       "      <td>1.917827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.008500</td>\n",
       "      <td>1.915435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLM training completed and model saved.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "pynvml does not seem to be installed or it can't be imported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpynvml\u001b[0m  \u001b[0;31m# type: ignore[import]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pynvml'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_827/340818044.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# 打印GPU使用情况\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"GPU utilization: {torch.cuda.utilization()}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"GPU memory cached: {torch.cuda.memory_cached() / 1e9:.2f} GB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mutilization\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[0mdepending\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproduct\u001b[0m \u001b[0mbeing\u001b[0m \u001b[0mqueried\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m     \"\"\"\n\u001b[0;32m--> 931\u001b[0;31m     \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_pynvml_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_nvml_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpynvml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnvmlDeviceGetHandleByIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_get_pynvml_handler\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_pynvml_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_HAS_PYNVML\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         raise ModuleNotFoundError(\n\u001b[0m\u001b[1;32m    886\u001b[0m             \u001b[0;34m\"pynvml does not seem to be installed or it can't be imported.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         ) from _PYNVML_ERR\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: pynvml does not seem to be installed or it can't be imported."
     ]
    }
   ],
   "source": [
    "# 分割数据集为训练集和验证集\n",
    "train_size = int(0.9 * len(tokenized_datasets))  # 计算训练集大小为数据集的90%\n",
    "train_dataset = tokenized_datasets.select(range(train_size))  # 选择前90%作为训练集\n",
    "eval_dataset = tokenized_datasets.select(range(train_size, len(tokenized_datasets)))  # 选择剩余10%作为验证集\n",
    "\n",
    "# 加载预训练的Masked Language Model\n",
    "model_checkpoint = \"distilbert-base-uncased\"  # 指定预训练模型\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)  # 从预训练模型加载Masked Language Model\n",
    "\n",
    "# 定义数据整理器\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,  # 指定分词器\n",
    "    mlm=True,  # 启用遮蔽语言模型任务\n",
    "    mlm_probability=0.15  # 设置遮蔽概率为15%\n",
    ")\n",
    "\n",
    "# 设置训练参数\n",
    "batch_size = 64  # 设置批次大小，可以根据需要调整\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./{model_checkpoint}-finetuned-MLM\",  # 设置输出目录\n",
    "    overwrite_output_dir=True,  # 允许覆盖输出目录\n",
    "    num_train_epochs=10,  # 设置训练轮数\n",
    "    eval_strategy=\"epoch\",  # 设置评估策略为每轮结束时\n",
    "    save_strategy=\"epoch\",  # 设置保存策略为每轮结束时\n",
    "    per_device_train_batch_size=32,  # 设置每个设备的训练批次大小\n",
    "    per_device_eval_batch_size=32,  # 设置每个设备的评估批次大小\n",
    "    gradient_accumulation_steps=2,  # 设置梯度累积步数，相当于批次大小为64\n",
    "    learning_rate=2e-5,  # 设置学习率\n",
    "    weight_decay=0.01,  # 设置权重衰减\n",
    "    save_total_limit=2,  # 设置最多保存模型数量\n",
    "    logging_dir=\"/root/tf-logs\",  # 设置日志目录\n",
    "    logging_strategy=\"steps\",  # 设置日志记录策略为每步\n",
    "    logging_steps=500,  # 设置每500步记录一次日志\n",
    "    push_to_hub=False,  # 不推送到huggingface hub\n",
    "    fp16=True,  # 启用混合精度训练\n",
    "    no_cuda=False  # 不禁用CUDA\n",
    ")\n",
    "\n",
    "# 定义Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,  # 指定模型\n",
    "    args=training_args,  # 指定训练参数\n",
    "    train_dataset=train_dataset,  # 指定训练数据集\n",
    "    eval_dataset=eval_dataset,  # 指定验证数据集\n",
    "    data_collator=data_collator,  # 指定数据整理器\n",
    "    tokenizer=tokenizer,  # 指定分词器\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "print(\"Starting MLM training...\")\n",
    "trainer.train()  # 执行训练\n",
    "\n",
    "# 保存模型和分词器\n",
    "trainer.save_model(\"./mlm_model\")  # 保存模型到指定目录\n",
    "tokenizer.save_pretrained(\"./mlm_model\")  # 保存分词器到指定目录\n",
    "\n",
    "print(\"MLM training completed and model saved.\")  # 打印训练完成信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eda18f-7ec4-44ec-8b3d-935267d5b0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75d2dc7e-c52a-4920-8ebc-b21755c469ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 禁用tokenizers并行化处理以避免警告和可能的死锁问题\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa074de6-36a6-40af-a29a-e41869f4b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入torch库\n",
    "import torch\n",
    "# 打印当前CUDA设备的索引号\n",
    "print(torch.cuda.current_device())\n",
    "# 打印可用的CUDA设备数量\n",
    "print(torch.cuda.device_count())\n",
    "# 打印第一个CUDA设备的名称\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26280c09-daa2-46d2-94b4-c50a9907f624",
   "metadata": {},
   "source": [
    "## 微调训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0833ae80-688d-434d-8e82-7c9dca49567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入torch库，用于深度学习\n",
    "import torch\n",
    "# 从torch库导入nn模块，用于构建神经网络\n",
    "from torch import nn\n",
    "# 从transformers库导入AutoModel, AutoTokenizer, TrainingArguments, Trainer, TrainerCallback，用于加载预训练模型和训练\n",
    "from transformers import AutoModel, AutoTokenizer, TrainingArguments, Trainer, TrainerCallback\n",
    "# 导入Dataset类，用于处理数据集\n",
    "from datasets import Dataset\n",
    "# 导入pandas库，用于数据处理和分析\n",
    "import pandas as pd\n",
    "# 从sklearn.model_selection导入train_test_split函数，用于分割数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 从sklearn.metrics导入accuracy_score, f1_score，用于评估模型性能\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# 导入matplotlib.pyplot库，用于绘图\n",
    "import matplotlib.pyplot as plt\n",
    "# 导入numpy库，用于数值计算\n",
    "import numpy as np\n",
    "\n",
    "# 以下导入重复，可以省略\n",
    "# from datasets import Dataset\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# 导入os库，用于操作系统功能，如文件路径操作\n",
    "import os\n",
    "# 从typing导入Dict, List, Tuple，用于类型注解，提高代码可读性\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60604536-8e03-4759-afd5-b30b3eee7ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入torch库，用于深度学习中的张量计算和自动微分\n",
    "import torch\n",
    "# 从transformers库导入AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "# AutoTokenizer用于自动加载预训练的分词器\n",
    "# AutoModelForSequenceClassification用于加载预训练的序列分类模型\n",
    "# TrainingArguments用于配置训练参数\n",
    "# Trainer用于训练模型\n",
    "# DataCollatorWithPadding用于批处理时自动填充序列到相同长度\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "# 导入Dataset类，用于处理数据集\n",
    "from datasets import Dataset\n",
    "# 从sklearn.model_selection导入train_test_split函数，用于分割数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 导入pandas库，用于数据处理和分析\n",
    "import pandas as pd\n",
    "# 导入numpy库，用于数值计算\n",
    "import numpy as np\n",
    "# 导入evaluate库，用于模型评估\n",
    "import evaluate\n",
    "# 从typing导入Dict，用于类型注解，提高代码可读性\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "227a4490-fe72-41a2-9193-bdaaea0dad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置设备为GPU如果可用，否则使用CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 根据指定的模型路径加载预训练的分词器\n",
    "model_name = \"./mlm_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 从指定路径加载训练数据集，并分割为训练集和验证集\n",
    "train_df = pd.read_csv('/root/data/train_set.csv', sep='\\t')\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "# 将训练集和验证集的DataFrame转换为Dataset对象\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebac71e2-68ec-407a-aa3a-ab94ebc93056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cb01a8312e434eb46685d92e5c532f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/180000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a43211fd664b1c8eafa39a617e2efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 定义一个函数用于对数据集中的文本进行分词\n",
    "def tokenize_function(examples):\n",
    "    # 对文本进行分词，设置最大长度为512，超出部分截断，不足部分填充\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# 使用定义的分词函数对训练数据集进行分词，移除原始文本列\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "# 使用定义的分词函数对验证数据集进行分词，移除原始文本列\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c914fa6a-51e7-4f0b-b1eb-a0fd33ae40a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at ./mlm_model and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# 定义注意力机制类，继承自nn.Module\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        # 定义注意力网络结构\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),  # 线性层\n",
    "            nn.Tanh(),  # Tanh激活函数\n",
    "            nn.Linear(hidden_size, 1),  # 线性层，输出维度为1\n",
    "            nn.Softmax(dim=1)  # Softmax激活函数，按维度1进行\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        # 计算注意力权重\n",
    "        attention_weights = self.attention(encoder_outputs)\n",
    "        # 计算上下文向量，通过权重加权求和\n",
    "        context_vector = torch.sum(attention_weights * encoder_outputs, dim=1)\n",
    "        return context_vector\n",
    "\n",
    "# 定义自定义的分类模型类，继承自nn.Module\n",
    "class CustomClassificationModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, num_labels):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels  # 类别数\n",
    "        # 加载预训练模型\n",
    "        self.base_model = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        # 实例化注意力机制\n",
    "        self.attention = Attention(self.base_model.config.hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)  # Dropout层\n",
    "        # 分类器线性层\n",
    "        self.classifier = nn.Linear(self.base_model.config.hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 通过基模型获取输出\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        # 通过注意力机制获取上下文向量\n",
    "        context_vector = self.attention(sequence_output)\n",
    "        # 应用dropout\n",
    "        pooled_output = self.dropout(context_vector)\n",
    "        # 通过分类器获取最终的logits\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# 定义计算评估指标的函数\n",
    "def compute_metrics(eval_pred: EvalPrediction) -> Dict[str, float]:\n",
    "    logits, labels = eval_pred\n",
    "    # 预测结果\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # 计算准确率\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    # 计算加权F1分数\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# 定义保存最佳模型的回调类\n",
    "class SaveBestModelCallback(TrainerCallback):\n",
    "    def __init__(self, save_path: str):\n",
    "        self.save_path = save_path  # 保存路径\n",
    "        self.best_f1 = 0.0  # 最佳F1分数\n",
    "\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        metrics = kwargs.get('metrics', {})\n",
    "        f1 = metrics.get('eval_f1', 0)\n",
    "        \n",
    "        # 如果当前F1分数超过之前的最佳分数，则保存模型\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            model = kwargs.get('model')\n",
    "            if model:\n",
    "                model.save_pretrained(os.path.join(self.save_path, f\"best_model_f1_{f1:.4f}\"))\n",
    "                print(f\"New best model saved with F1 score: {f1:.4f}\")\n",
    "        \n",
    "        return control\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/root/newsclassification/checkpoints\",  # 输出目录\n",
    "    eval_strategy=\"epoch\",  # 评估策略\n",
    "    learning_rate=2e-5,  # 学习率\n",
    "    per_device_train_batch_size=32,  # 训练批次大小\n",
    "    per_device_eval_batch_size=32,  # 评估批次大小\n",
    "    num_train_epochs=10,  # 训练轮数\n",
    "    weight_decay=0.01,  # 权重衰减\n",
    "    save_strategy=\"epoch\",  # 保存策略\n",
    "    save_total_limit=2,  # 最多保存模型数\n",
    "    load_best_model_at_end=True,  # 训练结束时加载最佳模型\n",
    "    metric_for_best_model=\"f1\",  # 选择最佳模型的指标\n",
    ")\n",
    "\n",
    "# 建立标签与ID的映射\n",
    "id2label = {0: '科技', 1: '股票', 2: '体育', 3: '娱乐', 4: '时政', 5: '社会', \n",
    "            6: '教育', 7: '财经', 8: '家居', 9: '游戏', 10: '房产', \n",
    "            11: '时尚', 12: '彩票', 13: '星座'}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# 加载预训练模型，设置分类数和标签映射\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"./mlm_model\",\n",
    "    num_labels=14,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "# 初始化Trainer，设置模型、训练参数、数据集、分词器、评估指标和回调函数\n",
    "save_best_model_callback = SaveBestModelCallback(save_path=\"/root/newsclassification/best_model\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[save_best_model_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7462bb5c-2477-4086-acc3-d6da66a4426b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56250' max='56250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56250/56250 5:48:23, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.180841</td>\n",
       "      <td>0.946550</td>\n",
       "      <td>0.946454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.144130</td>\n",
       "      <td>0.957350</td>\n",
       "      <td>0.957347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.103100</td>\n",
       "      <td>0.139039</td>\n",
       "      <td>0.961300</td>\n",
       "      <td>0.961253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>0.164554</td>\n",
       "      <td>0.959900</td>\n",
       "      <td>0.959851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.178419</td>\n",
       "      <td>0.962400</td>\n",
       "      <td>0.962410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.212027</td>\n",
       "      <td>0.961800</td>\n",
       "      <td>0.961810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.240215</td>\n",
       "      <td>0.960950</td>\n",
       "      <td>0.960927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.255313</td>\n",
       "      <td>0.961700</td>\n",
       "      <td>0.961683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.266968</td>\n",
       "      <td>0.961650</td>\n",
       "      <td>0.961631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.276260</td>\n",
       "      <td>0.961800</td>\n",
       "      <td>0.961766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with F1 score: 0.9465\n",
      "New best model saved with F1 score: 0.9573\n",
      "New best model saved with F1 score: 0.9613\n",
      "New best model saved with F1 score: 0.9624\n",
      "评估模型...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 01:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评估结果: {'eval_loss': 0.17841918766498566, 'eval_accuracy': 0.9624, 'eval_f1': 0.9624101825614323, 'eval_runtime': 82.0643, 'eval_samples_per_second': 243.711, 'eval_steps_per_second': 7.616, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "print(\"开始训练...\")\n",
    "trainer.train()\n",
    "\n",
    "# 评估模型\n",
    "print(\"评估模型...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"评估结果: {eval_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb1e29-655d-4908-9bc0-7e015fb5b8c4",
   "metadata": {},
   "source": [
    "## 推理部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9941e3cb-d80f-4c89-a274-e6bb00c73281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c666c4ff89764e44a2b51db9d7af3c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 设置设备，如果CUDA可用则使用GPU，否则使用CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 从预训练模型路径加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./mlm_model\")\n",
    "\n",
    "# 从指定路径加载测试数据集\n",
    "test_df = pd.read_csv('/root/data/test_a.csv', sep='\\t')\n",
    "# 将pandas DataFrame转换为Hugging Face的Dataset\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# 定义tokenize函数，用于处理数据集中的文本\n",
    "def tokenize_function(examples):\n",
    "    # 对文本进行tokenize，设置最大长度，超出部分进行截断\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# 对测试数据集应用tokenize函数，移除原始文本列\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f406362c-e015-41d3-8c2f-6788db2cc5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置数据集的格式为PyTorch张量，指定需要转换的列\n",
    "tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4721b1e6-b2d5-42f7-8085-f8194b853e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 1563/1563 [03:01<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 model predictions saved to /root/newsclassification/best_f1_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# # 定义预测函数\n",
    "# def predict(model, dataset):\n",
    "#     model.eval()\n",
    "#     dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "#     predictions = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "#             inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "#             outputs = model(**inputs)\n",
    "#             predictions.extend(outputs.logits.argmax(dim=-1).cpu().numpy())\n",
    "    \n",
    "#     return predictions\n",
    "\n",
    "\n",
    "# # 加载并预测：最佳F1模型\n",
    "# best_model = AutoModelForSequenceClassification.from_pretrained(\"/root/newsclassification/best_model/best_model_f1_0.9624\")\n",
    "# best_model.to(device)\n",
    "# best_predictions = predict(best_model, tokenized_test)\n",
    "\n",
    "# # 保存最佳F1模型的预测结果\n",
    "# best_submission = pd.DataFrame({'label': best_predictions})\n",
    "# best_submission.to_csv('/root/newsclassification/best_f1_submission.csv', index=False)\n",
    "# print(\"Best F1 model predictions saved to /root/newsclassification/best_f1_submission.csv\")\n",
    "\n",
    "# 定义预测函数\n",
    "def predict(model, dataset):\n",
    "    model.eval()  # 将模型设置为评估模式\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)  # 创建数据加载器\n",
    "    predictions = []  # 初始化预测结果列表\n",
    "    \n",
    "    with torch.no_grad():  # 不计算梯度，减少内存消耗\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):  # 遍历数据批次\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}  # 将数据移动到指定设备\n",
    "            outputs = model(**batch)  # 获取模型输出\n",
    "            predictions.extend(outputs.logits.argmax(dim=-1).cpu().numpy())  # 提取预测结果并转换为numpy数组\n",
    "    \n",
    "    return predictions  # 返回预测结果\n",
    "\n",
    "# 加载最佳F1模型\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained(\"/root/newsclassification/best_model/best_model_f1_0.9624\")\n",
    "best_model.to(device)  # 将模型移动到指定设备\n",
    "best_predictions = predict(best_model, tokenized_test)  # 使用最佳模型进行预测\n",
    "\n",
    "# 保存最佳F1模型的预测结果到CSV文件\n",
    "pd.DataFrame({'label': best_predictions}).to_csv('/root/newsclassification/best_f1_submission.csv', index=False)\n",
    "print(\"Best F1 model predictions saved to /root/newsclassification/best_f1_submission.csv\")  # 打印保存成功的消息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d3150adb-4d2e-4d67-a8df-3445e5a9c657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "191ad421-c5ee-44e5-ac9f-5a46ba576d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入PyTorch库\n",
    "import torch\n",
    "# 从PyTorch导入DataLoader，用于批量加载数据\n",
    "from torch.utils.data import DataLoader\n",
    "# 从transformers库导入AutoModelForSequenceClassification，用于序列分类任务\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# 导入datasets库的Dataset，用于处理数据集\n",
    "from datasets import Dataset\n",
    "# 导入pandas库，用于数据处理和CSV文件读写\n",
    "import pandas as pd\n",
    "# 导入tqdm库，用于显示进度条\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "410e2d94-454d-4d66-9b81-550cda5fd17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 1563/1563 [03:02<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final epoch model predictions saved to /root/newsclassification/final_epoch_submission.csv\n",
      "Prediction completed for both models.\n"
     ]
    }
   ],
   "source": [
    "# 加载最终epoch的模型用于预测\n",
    "final_model = AutoModelForSequenceClassification.from_pretrained(\"/root/newsclassification/checkpoints/checkpoint-56250\")\n",
    "final_model.to(device)  # 将模型移动到指定的设备上，比如GPU\n",
    "final_predictions = predict(final_model, tokenized_test)  # 使用预测函数进行预测\n",
    "\n",
    "# 将最终epoch模型的预测结果保存到CSV文件中\n",
    "pd.DataFrame({'label': final_predictions}).to_csv('/root/newsclassification/final_epoch_submission.csv', index=False)\n",
    "print(\"Final epoch model predictions saved to /root/newsclassification/final_epoch_submission.csv\")  # 打印保存成功的消息\n",
    "\n",
    "print(\"Prediction completed for both models.\")  # 打印完成预测的消息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "28e8018d-ffce-45f6-93f0-68897ee58354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints 文件夹中的文件: ['runs', 'checkpoint-28125', 'checkpoint-56250']\n",
      "best_model 文件夹中的文件: ['best_model_f1_0.9465', 'best_model_f1_0.9573', 'best_model_f1_0.9613', '.ipynb_checkpoints', 'best_model_f1_0.9624']\n"
     ]
    }
   ],
   "source": [
    "# 导入os模块，用于操作文件系统\n",
    "import os\n",
    "\n",
    "# 定义checkpoints文件夹的路径\n",
    "checkpoint_dir = \"/root/newsclassification/checkpoints\"\n",
    "# 定义best_model文件夹的路径\n",
    "best_model_dir = \"/root/newsclassification/best_model\"\n",
    "\n",
    "# 使用os.listdir列出checkpoints文件夹中的文件，并打印\n",
    "print(f\"checkpoints 文件夹中的文件: {os.listdir(checkpoint_dir)}\")\n",
    "\n",
    "# 使用os.listdir列出best_model文件夹中的文件，并打印\n",
    "print(f\"best_model 文件夹中的文件: {os.listdir(best_model_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a6310308-3038-4399-8c87-0fb5469feb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 1563/1563 [03:02<00:00,  8.59it/s]\n",
      "Predicting: 100%|██████████| 1563/1563 [03:03<00:00,  8.52it/s]\n",
      "Predicting: 100%|██████████| 1563/1563 [03:03<00:00,  8.51it/s]\n",
      "Predicting: 100%|██████████| 1563/1563 [03:03<00:00,  8.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# 定义模型路径列表\n",
    "model_paths = [\n",
    "    \"/root/newsclassification/checkpoints/checkpoint-56250\",\n",
    "    \"/root/newsclassification/checkpoints/checkpoint-28125\",\n",
    "    \"/root/newsclassification/best_model/best_model_f1_0.9613\",\n",
    "    \"/root/newsclassification/best_model/best_model_f1_0.9624\"\n",
    "]\n",
    "\n",
    "# 初始化用于存储所有模型预测结果的列表\n",
    "all_predictions = []\n",
    "\n",
    "# 遍历模型路径列表，加载每个模型并进行预测\n",
    "for path in model_paths:\n",
    "    # 从指定路径加载序列分类模型\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "    model.to(device)  # 将模型移动到指定设备\n",
    "    predictions = predict(model, tokenized_test)  # 调用预测函数获取预测结果\n",
    "    all_predictions.append(predictions)  # 将预测结果添加到列表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e99099d1-3ffd-4f95-983e-0bbbb5a6cf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After transpose:\n",
      "(50000, 4)\n",
      "Voting model predictions saved to /root/newsclassification/voting_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# 导入numpy和Counter类\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# 转置预测结果，使其形状为 (num_samples, num_models)\n",
    "all_predictions = np.array(all_predictions).T\n",
    "\n",
    "# 打印转置后的 all_predictions 的形状\n",
    "print(\"After transpose:\")\n",
    "print(all_predictions.shape)\n",
    "\n",
    "# 投票融合\n",
    "final_predictions = []\n",
    "\n",
    "for preds in all_predictions:\n",
    "    # 对每个样本的预测结果进行投票，选择出现次数最多的类别\n",
    "    most_common = Counter(preds).most_common(1)[0][0]\n",
    "    final_predictions.append(most_common)\n",
    "\n",
    "# 保存融合后的预测结果到CSV文件\n",
    "pd.DataFrame({'label': final_predictions}).to_csv('/root/newsclassification/voting_submission.csv', index=False)\n",
    "print(\"Voting model predictions saved to /root/newsclassification/voting_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df4f9a22-b235-4318-9fcd-1d839eb9f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5dfb65ba-c771-4221-bda1-91fd37a0bf86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 50000)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(all_predictions).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
